# backend/free_llm.py
import os
import httpx
import json
from typing import List, Dict, Any

# Tu token de Hugging Face
HF_TOKEN = "HUGGINGFACE_TOKEN_PLACEHOLDER"

async def call_free_llm(messages: List[Dict[str, str]], provider: str = "huggingface") -> str:
    """
    LLM gratuito alternativo - No modifica tu cÃ³digo existente
    """
    if provider == "huggingface":
        return await call_huggingface_chat(messages)
    else:
        return await call_huggingface_chat(messages)

async def call_huggingface_chat(messages: List[Dict[str, str]]) -> str:
    """
    Hugging Face Inference API - Modelo activo y gratuito
    """
    # MODELO ACTUALIZADO: Google Flan-T5 (siempre disponible)
    API_URL = "https://api-inference.huggingface.co/models/google/flan-t5-large"
    headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    
    # Extraer el Ãºltimo mensaje del usuario
    user_message = ""
    for msg in reversed(messages):
        if msg["role"] == "user":
            user_message = msg["content"]
            break
    
    if not user_message:
        return get_contextual_response("")
    
    # Preparar prompt para el modelo
    prompt = f"Responde como experto en desarrollo: {user_message}"
    
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_length": 300,
            "temperature": 0.7,
            "do_sample": True
        }
    }
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(API_URL, headers=headers, json=payload)
            
            if response.status_code == 200:
                result = response.json()
                
                # Procesar respuesta de Flan-T5
                if isinstance(result, list) and len(result) > 0:
                    generated_text = result[0].get("generated_text", "")
                    if generated_text and len(generated_text) > 10:
                        return f"ğŸ¤– [HF] {generated_text}"
                
                # Si no hay buena respuesta, usar contextual
                return get_contextual_response(user_message)
                    
            else:
                # Si hay error de API, usar respuestas contextuales inteligentes
                return get_contextual_response(user_message)

    except Exception as e:
        # En caso de cualquier error, usar respuestas predefinidas
        return get_contextual_response(user_message)

def get_contextual_response(user_message: str) -> str:
    """Respuestas inteligentes y especÃ­ficas cuando la API falla"""
    user_lower = user_message.lower()
    
    # Respuestas detalladas y especÃ­ficas por contexto
    responses = {
        "kubernetes": "ğŸš€ **Para aprender Kubernetes desde cero:**\n\n1. **Conceptos bÃ¡sicos:** Comprende Pods, Deployments, Services\n2. **InstalaciÃ³n:** Usa minikube para entorno local\n3. **PrÃ¡ctica:** Ejecuta `kubectl get pods`, `kubectl apply -f deployment.yaml`\n4. **Recursos:** Kubernetes.io documentation y Katacoda labs\n\nRecomiendo empezar con minikube y practicar con ejemplos simples.",
        
        "fastapi": "âš¡ **FastAPI para principiantes:**\n\n1. **InstalaciÃ³n:** `pip install fastapi uvicorn`\n2. **Primer API:** Crea app con `@app.get('/')`\n3. **CaracterÃ­sticas:** Tipado con Pydantic, documentaciÃ³n automÃ¡tica\n4. **EjecuciÃ³n:** `uvicorn main:app --reload`\n\nFastAPI es rÃ¡pido y tiene documentaciÃ³n interactiva en /docs.",
        
        "python": "ğŸ **Python desarrollo:**\n\nâ€¢ **Fundamentos:** Variables, funciones, clases\nâ€¢ **Estructuras:** Listas, diccionarios, comprehensions\nâ€¢ **Avanzado:** Decoradores, context managers, async/await\nâ€¢ **LibrerÃ­as:** Requests, Pandas, FastAPI, Django\n\nPractica con proyectos pequeÃ±os primero.",
        
        "docker": "ğŸ³ **Docker desde cero:**\n\n1. **Instalar Docker** en tu sistema\n2. **Dockerfile:** Define tu aplicaciÃ³n\n3. **Comandos:** `docker build -t myapp .`, `docker run -p 8000:8000 myapp`\n4. **Docker Compose** para mÃºltiples servicios\n\nComienza con contenedores simples y luego redes.",
        
        "hola": "ğŸ‘‹ **Â¡Hola! Soy tu asistente de desarrollo.**\n\nPuedo ayudarte con:\nâ€¢ ğŸ Python programming\nâ€¢ âš¡ FastAPI y desarrollo web\nâ€¢ ğŸš€ Kubernetes y DevOps\nâ€¢ ğŸ³ Docker y contenedores\nâ€¢ ğŸ—„ï¸ Bases de datos y APIs\n\nÂ¿En quÃ© tema te puedo asistir hoy?",
        
        "quÃ© puedes hacer": "ğŸ› ï¸ **Mis Ã¡reas de especializaciÃ³n:**\n\nâ€¢ **Backend Development:** FastAPI, Django, Flask\nâ€¢ **DevOps:** Kubernetes, Docker, CI/CD\nâ€¢ **Python:** ProgramaciÃ³n, librerÃ­as, best practices\nâ€¢ **APIs:** DiseÃ±o, documentaciÃ³n, seguridad\nâ€¢ **Bases de datos:** SQL, ORMs, optimizaciÃ³n\n\nÂ¿QuÃ© te interesa aprender o mejorar?",
        
        "gracias": "ğŸ˜Š **Â¡De nada! Estoy aquÃ­ para ayudarte.**\n\nSi tienes mÃ¡s preguntas sobre desarrollo, DevOps, o cualquier tema tÃ©cnico, no dudes en preguntar. Â¿Hay algo especÃ­fico en lo que te pueda ayudar ahora?"
    }
    
    # Buscar por palabras clave
    for keyword, response in responses.items():
        if keyword in user_lower:
            return response
    
    # Detectar contexto tÃ©cnico
    tech_keywords = {
        "program": "ğŸ’» **Desarrollo de software:** Practica con proyectos reales, estudia patrones de diseÃ±o, y contribuye a cÃ³digo abierto.",
        "code": "ğŸ“ **Escribir buen cÃ³digo:** EnfÃ³cate en cÃ³digo limpio, testing, y documentaciÃ³n. Practica daily.",
        "develop": "ğŸ‘¨â€ğŸ’» **Desarrollo profesional:** Aprende Git, metodologÃ­as Ã¡giles, y trabaja en equipo.",
        "api": "ğŸŒ **APIs:** DiseÃ±a RESTful APIs, documenta con OpenAPI, implementa autenticaciÃ³n y versionado.",
        "backend": "âš™ï¸ **Backend development:** Domina bases de datos, caching, seguridad, y escalabilidad.",
        "database": "ğŸ—„ï¸ **Bases de datos:** Aprende SQL, normalizaciÃ³n, Ã­ndices, y ORMs.",
        "server": "ğŸ–¥ï¸ **Servidores:** Estudia Linux, Nginx, administraciÃ³n, y monitoreo."
    }
    
    for keyword, response in tech_keywords.items():
        if keyword in user_lower:
            return response
    
    # Respuesta por defecto contextual
    if "?" in user_message or "cÃ³mo" in user_lower or "quÃ©" in user_lower:
        return f"ğŸ¤” **Sobre tu pregunta:** '{user_message}'\n\nTe recomiendo:\n1. DocumentaciÃ³n oficial del tema\n2. Tutoriales prÃ¡cticos paso a paso\n3. Proyectos hands-on para aprender haciendo\n4. Comunidades como Stack Overflow para dudas especÃ­ficas\n\nÂ¿Quieres que profundice en algÃºn aspecto en particular?"
    
    return f"ğŸ’¡ **Sobre:** '{user_message}'\n\nEn desarrollo, la prÃ¡ctica constante y proyectos reales son clave para el aprendizaje. Â¿Te interesa algÃºn tema especÃ­fico como Python, Kubernetes, FastAPI o Docker?"
